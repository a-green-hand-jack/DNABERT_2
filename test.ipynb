{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\PyTorchGpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    RobertaTokenizerFast,\n",
    "    BertForMaskedLM,\n",
    "    Trainer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    PreTrainedTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BertTokenizer,\n",
    "    BertForPreTraining, \n",
    "    BertTokenizerFast,\n",
    "    TrainingArguments,\n",
    "    BertConfig, \n",
    "    # LocalRationalAttention,\n",
    "    BertModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_convert_tokenizer(load_path: str) -> PreTrainedTokenizerFast:\n",
    "    \"\"\"\n",
    "    Load a tokenizer from a file and convert it to a PreTrainedTokenizerFast object.\n",
    "\n",
    "    Args:\n",
    "        load_path (str): Path to the tokenizer file.\n",
    "\n",
    "    Returns:\n",
    "        PreTrainedTokenizerFast: Converted PreTrainedTokenizerFast object.\n",
    "    \"\"\"\n",
    "    # new_tokenizer = Tokenizer.from_file(load_path)\n",
    "    print(f\"load tokenize's vocab.txt from {load_path}\")\n",
    "    tokenizer = BertTokenizer(vocab_file=load_path, do_lower_case=False) # 注意，这里一定要规定`do_lower_case=False`!!!!!\n",
    "    # print(new_tokenizer.mask_token)\n",
    "    \n",
    "    \n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_spaces(dna_sequence, interval):\n",
    "        # 初始化结果字符串\n",
    "        result = \"\"\n",
    "        \n",
    "        # 遍历 DNA 序列\n",
    "        for i, base in enumerate(dna_sequence):\n",
    "            # 每隔指定间隔插入一个空格\n",
    "            if i % interval == 0 and i != 0:\n",
    "                result += \" \"\n",
    "            # 添加当前碱基\n",
    "            result += base\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tokenize's vocab.txt from ./tokenizer/tokenizer-config/dnabert-config/bert-config-3/vocab.txt\n",
      "load tokenize's vocab.txt from ./tokenizer/tokenizer-config/dnabert-config/bert-config-6/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "low_dna_tokenizer = load_and_convert_tokenizer(\"./tokenizer/tokenizer-config/dnabert-config/bert-config-3/vocab.txt\" )\n",
    "high_dna_tokenizer = load_and_convert_tokenizer(\"./tokenizer/tokenizer-config/dnabert-config/bert-config-6/vocab.txt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATG CTC GTA GCT TTA CGG T\n",
      "['ATG', 'CTC', 'GTA', '[UNK]', 'TTA', 'CGG', '[UNK]']\n",
      "[12, 43, 57, 1, 25, 52, 1]\n"
     ]
    }
   ],
   "source": [
    "dna_sequence = \"ATGCTCGTAGCTTTACGGT\"\n",
    "dna_sequence_3 = insert_spaces(dna_sequence, 3)\n",
    "print(dna_sequence_3)\n",
    "tokens_low = low_dna_tokenizer.tokenize(dna_sequence_3)\n",
    "print(tokens_low)\n",
    "low_token_ids = low_dna_tokenizer.convert_tokens_to_ids(tokens_low)\n",
    "print(low_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATGCTC GTAGCT TTACGG T\n",
      "['ATGCTC', 'GTAGCT', 'TTACGG', '[UNK]']\n",
      "[491, 3390, 1332, 1]\n"
     ]
    }
   ],
   "source": [
    "dna_sequence = \"ATGCTCGTAGCTTTACGGT\"\n",
    "dna_sequence_6 = insert_spaces(dna_sequence, 6)\n",
    "print(dna_sequence_6)\n",
    "tokens_high = high_dna_tokenizer.tokenize(dna_sequence_6)\n",
    "print(tokens_high)\n",
    "high_token_ids = high_dna_tokenizer.convert_tokens_to_ids(tokens_high)\n",
    "print(high_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def subtract_value_for_values(tensor, value_to_subtract):\n",
    "        \"\"\"对张量中小于等于 value_to_subtract 的值进行减法操作\"\"\"\n",
    "        subtracted_tensor = tensor.clone()  # 复制输入张量，以避免修改原始张量\n",
    "        \n",
    "        # 使用 torch.where() 函数将小于等于 value_to_subtract 的值替换为相应的减法结果\n",
    "        subtracted_tensor = torch.where(subtracted_tensor <= 5, \n",
    "                                        subtracted_tensor - value_to_subtract, \n",
    "                                        subtracted_tensor)\n",
    "        \n",
    "        return subtracted_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4051, 4082, 4096,    1, 4064, 4091,    1])\n",
      "tensor([ 491, 3390, 1332,    1])\n",
      "tensor([ 491, 3390, 1332,    1, 4051, 4082, 4096,    1, 4064, 4091,    1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 将token ID转换为tensor\n",
    "high_token_tensor = torch.tensor(high_token_ids, dtype=torch.long)\n",
    "low_token_tensor = subtract_value_for_values(torch.tensor(low_token_ids, dtype=torch.long), len(high_dna_tokenizer.vocab)) + torch.tensor(len(high_dna_tokenizer.vocab), dtype=torch.long)\n",
    "print(low_token_tensor)\n",
    "print(high_token_tensor)\n",
    "# 使用torch.cat()函数连接两个tensor\n",
    "combined_tensor = torch.cat((high_token_tensor, low_token_tensor), dim=0)\n",
    "print(combined_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n",
    "# from transformers.trainer_pt_utils import LabelSmoother\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers\n",
    "from typing import Optional, Dict, Sequence, Tuple, List, Union, Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset, ConcatDataset\n",
    "import dataclasses\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForMLM(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, instances: Sequence[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Collate function for masked language modeling.\n",
    "\n",
    "        Args:\n",
    "            instances (Sequence[Dict[str, torch.Tensor]]): List of instances containing input tensors.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: Dictionary containing input_ids, labels, and attention_mask tensors.\n",
    "        \"\"\"\n",
    "        instances = [instance['input_ids'] for instance in instances]\n",
    "        # print(instances)\n",
    "        # print(self.tokenizer.pad_token_id)\n",
    "        # import torch\n",
    "        inputs = pad_sequence(\n",
    "            instances,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        input_ids, labels, attention_masks = self.mask_tokens(inputs)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=attention_masks,\n",
    "        )\n",
    "    \n",
    "    def mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels/attention_mask for masked language modeling: 80% MASK, 10% random, 10%\n",
    "        original. N-gram not applied yet.\n",
    "        \"\"\"\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n",
    "                \" --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        # probability be `1` (masked), however in albert model attention mask `0` means masked, revert the value\n",
    "        attention_mask = (~masked_indices).float()\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            attention_padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            attention_mask.masked_fill_(attention_padding_mask, value=1.0)\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens, -100 is default for CE compute\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels, attention_mask\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tokenize's vocab.txt from ./tokenizer/tokenizer-config/dnabert-config/high-low-63-vocab.txt\n",
      "Processed data: {'input_ids': tensor([[ 491, 3390, 1332,    1, 4051, 4082,    4,    1, 4064, 4091,    1]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, 4096, -100, -100, -100, -100]]), 'attention_mask': tensor([[1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.]])}\n",
      "Decoded text: ['ATGCTC GTAGCT TTACGG ATG CTC TTA CGG']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BertModel.forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 打印model的输出\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./zhihan1996/DNA_bert_6\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOut put of the model\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\PyTorchGpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: BertModel.forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "# 使用 data_collator 处理数据\n",
    "dna_tokenizer = load_and_convert_tokenizer(\"./tokenizer/tokenizer-config/dnabert-config/high-low-63-vocab.txt\")\n",
    "data_collator = DataCollatorForMLM(tokenizer=dna_tokenizer, mlm=True, mlm_probability=0.15)\n",
    "# 使用data collator处理数据\n",
    "processed_data = data_collator([{\"input_ids\":combined_tensor}])\n",
    "\n",
    "# 打印处理后的数据\n",
    "print(\"Processed data:\", processed_data)\n",
    "# 解码处理后的数据\n",
    "decoded_text = dna_tokenizer.batch_decode(processed_data['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "# 打印解码后的文本\n",
    "print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "# 打印model的输出\n",
    "model = BertForPreTraining.from_pretrained(\"./zhihan1996/DNA_bert_6\")\n",
    "output = model(**processed_data)\n",
    "print(\"Out put of the model\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 4101])\n"
     ]
    }
   ],
   "source": [
    "print(output.prediction_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode text\n",
    "text = [\"hello\", \"world\"]\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "input_ids = encoded_input['input_ids']\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model.embeddings(input_ids=input_ids)\n",
    "\n",
    "# outputs now contains the embeddings\n",
    "print(outputs.shape)  # This should show the shape as (batch_size, sequence_length, 768) for BERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 7592,  102],\n",
      "        [ 101, 2088,  102]])\n",
      "{'input_ids': tensor([[ 101, 7592,  102],\n",
      "        [ 101, 2088,  102]]), 'token_type_ids': tensor([[0, 0, 0],\n",
      "        [0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1],\n",
      "        [1, 1, 1]])}\n",
      "tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [ 0.3739, -0.0156, -0.2456,  ..., -0.0317,  0.5514, -0.5241],\n",
      "         [-0.4815, -0.0189,  0.0092,  ..., -0.2806,  0.3895, -0.2815]],\n",
      "\n",
      "        [[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [ 0.7955,  0.9768,  0.0525,  ..., -0.1027,  0.6043, -0.4444],\n",
      "         [-0.4815, -0.0189,  0.0092,  ..., -0.2806,  0.3895, -0.2815]]])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(encoded_input)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorchGpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
