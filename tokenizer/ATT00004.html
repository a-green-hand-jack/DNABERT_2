<html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"></head><body style="overflow-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"><div></div><div><br></div><div>As for the model pre-training, we use the implementation provided by MosaicML.&nbsp;<a href="https://github.com/mosaicml/examples/tree/main/examples/benchmarks/bert">https://github.com/mosaicml/examples/tree/main/examples/benchmarks/bert</a>&nbsp;. You should be able to get the same model by following this script, but it needs some extra works to process the data into certain formats. You can also use HuggingFace example&nbsp;<a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling">https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling</a>&nbsp;by initialize the model, tokenizer, and config class with the codes on our Model Hub&nbsp;<a href="https://huggingface.co/zhihan1996/DNABERT-2-117M/tree/main">https://huggingface.co/zhihan1996/DNABERT-2-117M/tree/main</a>&nbsp;. This also results in a similar model.</div><div><br></div><div>We plan to share the codes for model pre-training. Yet cleaning up the codes is time consuming and I am currently blocking by other things with high priority. Will let you know once it’s ready.</div><div><br></div><div><br id="lineBreakAtBeginningOfMessage"><div>
<div dir="auto" style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; word-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"><div dir="auto" style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; word-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"><div dir="auto" style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; word-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"><div dir="auto" style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; word-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"><div dir="auto" style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; word-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;">Best,</div><div dir="auto" style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; word-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;">Zhihan</div><div dir="auto" style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; word-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;">--</div><div dir="auto" style="caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none; word-wrap: break-word; -webkit-nbsp-mode: space; line-break: after-white-space;">Zhihan Zhou (Ph.D. Candidate),<div>Department of Computer Science,</div><div>Northwestern University,</div><div>Evanston, IL, USA</div><div><a href="https://zhihan1996.github.io/">Personal Website</a></div></div></div><br class="Apple-interchange-newline"></div><br class="Apple-interchange-newline"></div><br class="Apple-interchange-newline"></div><br class="Apple-interchange-newline"><br class="Apple-interchange-newline">
</div>
<div><br><blockquote type="cite"><div>On Mar 22, 2024, at 23:38, jieke.wu@mail.ustc.edu.cn wrote:</div><br class="Apple-interchange-newline"><div><p class="MsoNormal"><font size="4" style="" face="Times New Roman">Dear Zhihan Zhou,</font></p><p class="MsoNormal"><font size="4" face="Times New Roman">I hope this email finds you well!</font></p><p class="MsoNormal"><font size="4" face="Times New Roman">I am Jieke Wu, an undergraduate student at USTC. I recently read your DNABERT-2 paper, greatly admired it, and would like to extend my congratulations on the impressive results achieved!</font></p><p class="MsoNormal"><font size="4" face="Times New Roman">Currently, I am working on the DNABERT-2 code in my research. I appreciate your provision of the fine-tuning code. However, I noticed that the code for pretraining the model and for training the BPE Tokenizer has not been made publicly available. I am particularly interested in these aspects, and I am wondering if you have any planned timeline for releasing this code?</font></p><p class="MsoNormal"><font size="4" face="Times New Roman">Furthermore, I am curious about how much time it took you to train the BPE Tokenizer?</font></p><p class="MsoNormal"><font size="4" face="Times New Roman">Thank you for considering my queries. I look forward to your response.</font></p><p class="MsoNormal"><font size="4" face="Times New Roman">Best regards,<o:p></o:p></font></p><p class="MsoNormal"><font size="4" style="" face="Times New Roman">Jieke Wu</font><span style="mso-spacerun:'yes';font-family:Calibri;mso-fareast-font-family:宋体;
mso-bidi-font-family:'Times New Roman';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><o:p></o:p></span></p></div></blockquote></div><br></div></body></html>