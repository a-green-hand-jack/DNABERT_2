{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    LineByLineTextDataset,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    RobertaTokenizerFast,\n",
    "    BertForMaskedLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    PreTrainedTokenizerFast,\n",
    "    PreTrainedTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BertTokenizer,\n",
    "    BertForPreTraining, \n",
    "    BertTokenizerFast\n",
    ")\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers\n",
    "from typing import Optional, Dict, Sequence, Tuple, List, Union, Any\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset, ConcatDataset\n",
    "import dataclasses\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tokenize's vocab.txt from ./dnabert-config/bert-config-6/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "def load_bert_and_save_tokenizer(load_path: str, output_dir:str, tokenizer_name:str) -> PreTrainedTokenizerFast:\n",
    "    \"\"\"\n",
    "    Load a tokenizer from a file and convert it to a PreTrainedTokenizerFast object.\n",
    "\n",
    "    Args:\n",
    "        load_path (str): Path to the tokenizer file.\n",
    "        output_dir (str): Directory to save the tokenizer JSON file.\n",
    "        tokenizer_name (str): Name of the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        PreTrainedTokenizerFast: Converted PreTrainedTokenizerFast object.\n",
    "    \"\"\"\n",
    "    # new_tokenizer = Tokenizer.from_file(load_path)\n",
    "    print(f\"load tokenize's vocab.txt from {load_path}\")\n",
    "    tokenizer = BertTokenizer(vocab_file=load_path, do_lower_case=False) # 注意，这里一定要规定`do_lower_case=False`!!!!!\n",
    "    # print(new_tokenizer.mask_token)\n",
    "    transformer_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer, mask_token = \"[MASK]\", padding_token = \"[PAD]\")\n",
    "\n",
    "    # 将 output_dir 转换为 Path 对象\n",
    "    output_dir_path = Path(output_dir)\n",
    "    \n",
    "    # 保存 tokenizer 到 JSON 文件\n",
    "    tokenizer.save_pretrained(output_dir_path )\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "high_model_name_or_path = \"./dnabert-config/bert-config-6/vocab.txt\"\n",
    "save_high_tokenizer_path = './dnabert-config/bert-token-6/'\n",
    "save_high_tokenizer_name = 'high_tonkenizer-6'\n",
    "high_dna_tokenizer = load_bert_and_save_tokenizer(high_model_name_or_path, save_high_tokenizer_path, save_high_tokenizer_name)\n",
    "# low_model_name_or_path = \"./dnabert-config/bert-config-3/vocab.txt\"\n",
    "# save_low_tokenizer_path = './dnabert-config/bert-token-3/'\n",
    "# save_low_tokenizer_name = 'high_tonkenizer-3'\n",
    "# low_dna_tokenizer = load_bert_and_save_tokenizer(low_model_name_or_path, save_low_tokenizer_path, save_low_tokenizer_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
